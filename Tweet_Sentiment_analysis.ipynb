{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0f8cf18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/priyankamanam/opt/anaconda3/lib/python3.9/site-packages/seaborn/rcmod.py:82: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(mpl.__version__) >= \"3.0\":\n",
      "/Users/priyankamanam/opt/anaconda3/lib/python3.9/site-packages/setuptools/_distutils/version.py:351: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import glob\n",
    "import os\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from emot.emo_unicode import UNICODE_EMOJI\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "pyLDAvis.enable_notebook()\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#nltk.download('punkt')\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bcde91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = r\"Tweets\"\n",
    "team_tweets_df= pd.DataFrame()\n",
    "team_tweets_df= pd.concat(map(pd.read_csv,glob.glob(os.path.join(data_files,'Team*.csv'))), ignore_index= True)\n",
    "team_tweets_df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28f20be9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Team</th>\n",
       "      <th>Date</th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Boston Bruins</td>\n",
       "      <td>2022-03-24 18:13:29+00:00</td>\n",
       "      <td>RT @SassTreasures: Check out this listing I ju...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Boston Bruins</td>\n",
       "      <td>2022-03-24 18:10:39+00:00</td>\n",
       "      <td>RT @tmlfaninvan: the Toronto Maple Leafs colla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Boston Bruins</td>\n",
       "      <td>2022-03-24 18:10:03+00:00</td>\n",
       "      <td>Check out this listing I just added to my #Pos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Boston Bruins</td>\n",
       "      <td>2022-03-24 18:02:10+00:00</td>\n",
       "      <td>@LeafsLindsay @ClendelWark Win for us is Tampa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Boston Bruins</td>\n",
       "      <td>2022-03-24 18:01:46+00:00</td>\n",
       "      <td>Today's NHL Picks: 7:00 PM ET - Tampa Bay Ligh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Boston Bruins</td>\n",
       "      <td>2022-03-24 18:00:20+00:00</td>\n",
       "      <td>Just one point separates the Boston Bruins and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Boston Bruins</td>\n",
       "      <td>2022-03-24 17:48:25+00:00</td>\n",
       "      <td>RT @BOSTONSPORTSB: Boston Sports rankings (cur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Boston Bruins</td>\n",
       "      <td>2022-03-24 17:48:07+00:00</td>\n",
       "      <td>RT @BOSTONSPORTSB: Boston Sports rankings (cur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Boston Bruins</td>\n",
       "      <td>2022-03-24 17:44:07+00:00</td>\n",
       "      <td>RT @cupofchowdah: Hampus Lindholm makes his Br...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Boston Bruins</td>\n",
       "      <td>2022-03-24 17:41:12+00:00</td>\n",
       "      <td>Hampus Lindholm makes his Bruins’ debut tonigh...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Team                       Date  \\\n",
       "0  Boston Bruins  2022-03-24 18:13:29+00:00   \n",
       "1  Boston Bruins  2022-03-24 18:10:39+00:00   \n",
       "2  Boston Bruins  2022-03-24 18:10:03+00:00   \n",
       "3  Boston Bruins  2022-03-24 18:02:10+00:00   \n",
       "4  Boston Bruins  2022-03-24 18:01:46+00:00   \n",
       "5  Boston Bruins  2022-03-24 18:00:20+00:00   \n",
       "6  Boston Bruins  2022-03-24 17:48:25+00:00   \n",
       "7  Boston Bruins  2022-03-24 17:48:07+00:00   \n",
       "8  Boston Bruins  2022-03-24 17:44:07+00:00   \n",
       "9  Boston Bruins  2022-03-24 17:41:12+00:00   \n",
       "\n",
       "                                               Tweet  \n",
       "0  RT @SassTreasures: Check out this listing I ju...  \n",
       "1  RT @tmlfaninvan: the Toronto Maple Leafs colla...  \n",
       "2  Check out this listing I just added to my #Pos...  \n",
       "3  @LeafsLindsay @ClendelWark Win for us is Tampa...  \n",
       "4  Today's NHL Picks: 7:00 PM ET - Tampa Bay Ligh...  \n",
       "5  Just one point separates the Boston Bruins and...  \n",
       "6  RT @BOSTONSPORTSB: Boston Sports rankings (cur...  \n",
       "7  RT @BOSTONSPORTSB: Boston Sports rankings (cur...  \n",
       "8  RT @cupofchowdah: Hampus Lindholm makes his Br...  \n",
       "9  Hampus Lindholm makes his Bruins’ debut tonigh...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# team_tweets_df = pd.read_csv(\"Team_tweets_17_03_2020.csv\")\n",
    "# team_tweets_df.shape\n",
    "team_tweets_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd59d0de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "efe8ea41",
   "metadata": {},
   "source": [
    "# Tweets cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0447bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sf/sxg31ljs3ks0h1bsflmz2z1c0000gn/T/ipykernel_1917/1076732081.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  team_tweets_df[\"Tweet\"] = team_tweets_df[\"Tweet\"].str.replace(r'https?:\\/\\/.*[\\r\\n]*', '').str.strip()\n",
      "/var/folders/sf/sxg31ljs3ks0h1bsflmz2z1c0000gn/T/ipykernel_1917/1076732081.py:5: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  team_tweets_df[\"Tweet\"] = team_tweets_df[\"Tweet\"].str.replace(r'@\\w+|#\\w+|RT|', '').str.strip()\n"
     ]
    }
   ],
   "source": [
    "#removing hyoerlinks\n",
    "team_tweets_df[\"Tweet\"] = team_tweets_df[\"Tweet\"].str.replace(r'https?:\\/\\/.*[\\r\\n]*', '').str.strip()\n",
    "\n",
    "#remove # and @ from tweets\n",
    "team_tweets_df[\"Tweet\"] = team_tweets_df[\"Tweet\"].str.replace(r'@\\w+|#\\w+|RT|', '').str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c41d2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for converting emojis into word\n",
    "def handle_emojis(tweet):\n",
    "    for emot in UNICODE_EMOJI:\n",
    "        #tweet = tweet.replace(emot, (\" \" + \"_\".join(UNICODE_EMOJI[emot].replace(\",\",\"\").replace(\":\",\"\").split())))\n",
    "        tweet = tweet.replace(emot, \"\")\n",
    "    return tweet\n",
    "\n",
    "team_tweets_df[\"Tweet\"] = team_tweets_df[\"Tweet\"].apply(handle_emojis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be28ee7a",
   "metadata": {},
   "source": [
    "# Sentiment Stats of each team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d44da530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1800, 4)\n",
      "(70286, 4)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "team_names = team_tweets_df[\"Team\"].unique()\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "Team_arr = []\n",
    "positive_scores = []\n",
    "negative_scores = []\n",
    "neutral_scores = []\n",
    "\n",
    "def determine_polarity(tweet):\n",
    "    pol_scores = analyzer.polarity_scores(tweet) \n",
    "    compound_score = pol_scores[\"compound\"]\n",
    "    if compound_score >= 0.5:\n",
    "        return 1\n",
    "    elif compound_score <= -0.5:\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "train_test_df = pd.DataFrame()\n",
    "train_df = pd.DataFrame()\n",
    "test_df = pd.DataFrame()\n",
    "pos_df = pd.DataFrame()\n",
    "neg_df = pd.DataFrame()\n",
    "neu_df = pd.DataFrame()\n",
    "\n",
    "for team in team_names:\n",
    "    curr_team_df = team_tweets_df.loc[team_tweets_df[\"Team\"] == team]\n",
    "    curr_team_df[\"polarity\"] = curr_team_df[\"Tweet\"].apply(determine_polarity)\n",
    "    pos_df = pos_df.append(curr_team_df[curr_team_df[\"polarity\"]== 1])\n",
    "    neg_df = neg_df.append(curr_team_df[curr_team_df[\"polarity\"]== -1])\n",
    "    neu_df = neu_df.append(curr_team_df[curr_team_df[\"polarity\"]== 0])\n",
    "\n",
    "train_df = train_df.append(pos_df.iloc[0:600, :])    \n",
    "train_df = train_df.append(neg_df.iloc[0:600, :])    \n",
    "train_df = train_df.append(neu_df.iloc[0:600, :]) \n",
    "\n",
    "test_df = test_df.append(pos_df.iloc[600:, :])    \n",
    "test_df = test_df.append(neg_df.iloc[600:, :])    \n",
    "test_df = test_df.append(neu_df.iloc[600:, :]) \n",
    "\n",
    "print(train_df.shape)\n",
    "print(test_df.shape)\n",
    "#     pos_num = curr_team_df[curr_team_df[\"polarity\"]== 1].count()[\"Tweet\"]\n",
    "#     neg_num = curr_team_df[curr_team_df[\"polarity\"]== -1].count()[\"Tweet\"]\n",
    "#     neu_num = curr_team_df[curr_team_df[\"polarity\"]== 0].count()[\"Tweet\"]\n",
    "    #curr_team_df[curr_team_df[\"polarity\"]== \"negative\"].to_csv(\"checking_neg.csv\")\n",
    "    #time.sleep(1000)\n",
    "#     Team_arr.append(team)\n",
    "#     positive_scores.append(pos_num)\n",
    "#     negative_scores.append(neg_num)\n",
    "#     neutral_scores.append(neu_num)\n",
    "    \n",
    "#     print(\"\\n=========== \",team ,\" Tweets Sentiment Stats: =================\")\n",
    "#     print(\"Positive Tweets: \", pos_num)\n",
    "#     print(\"Negative Tweets: \", neg_num)\n",
    "#     print(\"Neutral Tweets:  \", neu_num)\n",
    "\n",
    "# Team_sentiment_stats_df = pd.DataFrame({\"Team\" : Team_arr,\n",
    "#                               \"Positive_tweet\" : positive_scores,\n",
    "#                               \"Negative_tweet\" : negative_scores,\n",
    "#                               \"Neutral_tweet\" : neutral_scores\n",
    "#                                        })\n",
    "# print(Team_sentiment_stats_df)\n",
    "\n",
    "# Team_sentiment_stats_df.to_csv('Teams_sentiment.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca7fa3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install fastai -c fastai -c pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1df72f78",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/priyankamanam/opt/anaconda3/lib/python3.9/site-packages/tables/req_versions.py:20: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  min_numpy_version = LooseVersion('1.9.3')\n",
      "/Users/priyankamanam/opt/anaconda3/lib/python3.9/site-packages/tables/req_versions.py:21: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  min_numexpr_version = LooseVersion('2.6.2')\n",
      "/Users/priyankamanam/opt/anaconda3/lib/python3.9/site-packages/tables/req_versions.py:22: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  min_hdf5_version = LooseVersion('1.8.4')\n",
      "/Users/priyankamanam/opt/anaconda3/lib/python3.9/site-packages/tables/req_versions.py:23: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  min_blosc_version = LooseVersion(\"1.4.1\")\n",
      "/Users/priyankamanam/opt/anaconda3/lib/python3.9/site-packages/tables/req_versions.py:24: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  min_blosc_bitshuffle_version = LooseVersion(\"1.8.0\")\n",
      "/Users/priyankamanam/opt/anaconda3/lib/python3.9/site-packages/tables/filters.py:27: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  blosc_version = LooseVersion(tables.which_lib_version(\"blosc\")[1])\n",
      "/Users/priyankamanam/opt/anaconda3/lib/python3.9/site-packages/tables/tests/common.py:36: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  hdf5_version = LooseVersion(tables.hdf5_version)\n",
      "/Users/priyankamanam/opt/anaconda3/lib/python3.9/site-packages/tables/tests/common.py:37: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  blosc_version = LooseVersion(tables.which_lib_version(\"blosc\")[1])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'AWD_LSTM' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/sf/sxg31ljs3ks0h1bsflmz2z1c0000gn/T/ipykernel_1917/2954357109.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfastai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfastai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearner\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlanguage_model_learner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtweet_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlanguage_model_learner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAWD_LSTM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_mult\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'AWD_LSTM' is not defined"
     ]
    }
   ],
   "source": [
    "from fastai.text import *\n",
    "from fastai.text.learner import language_model_learner\n",
    "tweet_model = language_model_learner(train_df, AWD_LSTM, drop_mult=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a97dd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "font = {'family': 'serif',\n",
    "        'color':  'darkred',\n",
    "        'weight': 'normal',\n",
    "        'size': 14,\n",
    "        }\n",
    "\n",
    "team_total = range(32)\n",
    "fig = plt.figure(figsize = (10,6))\n",
    "ax = plt.subplot(111)\n",
    "plt.xlabel('Teams')\n",
    "plt.ylabel('Positive / Negative Tweets')\n",
    "\n",
    "#print(len(Team_sentiment_stats_df[\"Positive_tweet\"].tolist()))\n",
    "#print(len(Team_sentiment_stats_df[\"Team\"].tolist()))\n",
    "#print(len(Team_sentiment_stats_df[\"Negative_tweet\"].tolist()))\n",
    "\n",
    "n, bins, patches = plt.hist(Team_sentiment_stats_df[\"Team\"].tolist(), Team_sentiment_stats_df[\"Team\"].unique().__len__(), stacked=True, density=False)\n",
    "plt.title(\"Sentiments Both in same direction\", fontdict= font, loc='center')\n",
    "#print(bins)\n",
    "plt.xticks(ticks = bins[1:], labels = Team_sentiment_stats_df[\"Team\"], rotation = 90, horizontalalignment = 'left')\n",
    "ax.bar(team_total, Team_sentiment_stats_df[\"Positive_tweet\"].tolist(), width=1,alpha=1, color='b',facecolor='blue', edgecolor='black')\n",
    "ax.bar(team_total, Team_sentiment_stats_df[\"Negative_tweet\"].tolist(), width=1, alpha=0.4,color='r',facecolor='red', edgecolor='black')\n",
    "plt.show()\n",
    "#plt.hist(x, density=False)  # density=False would make counts\n",
    "#plt.ylabel('Probability')\n",
    "#plt.xlabel('Data')\n",
    "\n",
    "fig = plt.figure(figsize = (10,6))\n",
    "ax = plt.subplot(111)\n",
    "plt.xlabel('Teams')\n",
    "plt.ylabel('Positive / Negative Tweets')\n",
    "\n",
    "n, bins, patches = plt.hist(Team_sentiment_stats_df[\"Team\"].tolist(), Team_sentiment_stats_df[\"Team\"].unique().__len__(), stacked=True, density=False)\n",
    "negative_data_opposite = [i * -1 for i in Team_sentiment_stats_df[\"Negative_tweet\"].tolist()]\n",
    "\n",
    "plt.xticks(ticks=bins[1:], labels=Team_sentiment_stats_df[\"Team\"], rotation=90, horizontalalignment='left')\n",
    "plt.title(\"Sentiments Negative Tweets in opposite direction\", fontdict= font, loc='center')\n",
    "ax.bar(team_total, Team_sentiment_stats_df[\"Positive_tweet\"].tolist(), width=1, color='b',facecolor='blue', edgecolor='black')\n",
    "ax.bar(team_total, negative_data_opposite, alpha=1, width=1, color='r', facecolor='red', edgecolor='black')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c139c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing\n",
    "\"\"\"\n",
    "pos_tweets = [('I love this you car @hello', 'positive'),\n",
    "    ('This view is amazing', 'positive'),\n",
    "    ('I feel great this morning', 'positive'),\n",
    "    ('I am so excited #1234 about the concert', 'positive'),\n",
    "    ('He is my best friend', 'positive')]\n",
    "\n",
    "test = pd.DataFrame(pos_tweets)\n",
    "test.columns = [\"tweet\",\"class\"]\n",
    "\n",
    "test['tweet'] = test['tweet'].apply(stop_word_removal)\n",
    "print(test)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76564840",
   "metadata": {},
   "source": [
    "# Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed24addf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove punctuation\n",
    "team_tweets_df[\"Tweet\"] = team_tweets_df[\"Tweet\"].str.replace('[{}]'.format(string.punctuation), '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bbee05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove stop words\n",
    "\n",
    "stop_wrds = stopwords.words('english')\n",
    "\n",
    "def stop_word_removal(tweet):\n",
    "    return ' '.join([word for word in tweet.split() if word not in stop_wrds])\n",
    "\n",
    "team_tweets_df[\"Tweet\"] = team_tweets_df[\"Tweet\"].apply(stop_word_removal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a8671a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenization and stemming\n",
    "##need to change it to lemmatization\n",
    "tokenizer = TweetTokenizer(\n",
    "    preserve_case=False,\n",
    "    strip_handles=True,\n",
    "    reduce_len=True)\n",
    "\n",
    "snowball_stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def tokenization_stemming(tweet):\n",
    "    sentence_token_stem = []\n",
    "    token_sent = tokenizer.tokenize(tweet)\n",
    "    for token in token_sent:\n",
    "        sentence_token_stem.append(snowball_stemmer.stem(token))\n",
    "    #disabled stemming\n",
    "    return token_sent\n",
    "\n",
    "team_tweets_df[\"Tweet\"] = team_tweets_df[\"Tweet\"].apply(tokenization_stemming)\n",
    "# team_tweets_df.to_csv('check.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2dedec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatization\n",
    "import spacy \n",
    "\n",
    "# bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "# trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "# bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "# trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "def remove_team_name(tweet):\n",
    "    split_team = str.lower(sample_team_name).split()\n",
    "    split_team.append(split_team[-1][:-1])\n",
    "    #print(split_team)\n",
    "    return ' '.join([word for word in tweet if word not in split_team])\n",
    "    \n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    #bigram = gensim.models.Phrases(texts, min_count=2, threshold=10) # higher threshold fewer phrases.\n",
    "    #trigram = gensim.models.Phrases(bigram[texts], min_count=3, threshold=10)  \n",
    "    #bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    #trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "    #texts_out_bi = [bigram_mod[doc] for doc in texts]\n",
    "    split_team = str.lower(sample_team_name).split()\n",
    "    split_team.append(split_team[-1][:-1])\n",
    "    #print(texts_out_bi)\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        #texts_out.append([token.lemma_ if token.lemma_ not in ['-PRON-'] else '' for token in doc if token.pos_ in allowed_postags])\n",
    "        texts_out.append(\" \".join([token.lemma_ for token in doc if (token.pos_ in allowed_postags) and (token.lemma_ not in split_team)]))\n",
    "    return texts_out\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# Run in terminal: python3 -m spacy download en\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Do lemmatization keeping only Noun, Adj, Verb, Adverb\n",
    "sample_team_name = \"Boston Bruins\"\n",
    "sample_team_df =  team_tweets_df.loc[team_tweets_df[\"Team\"] == sample_team_name]\n",
    "#sample_team_df.head(5)\n",
    "#sample_team_df[\"Tweet\"] = sample_team_df[\"Tweet\"].apply(remove_team_name)\n",
    "sample_team_df[\"Tweet\"] = lemmatization(sample_team_df[\"Tweet\"], allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "#team_tweets_df[\"Tweet\"] = team_tweets_df[\"Tweet\"].apply(tokenization_stemming)\n",
    "#print(data_lemmatized[:2])\n",
    "sample_team_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0aeeef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LDA Topic Modelling\n",
    "#needs improvement\n",
    "#sample_team_df =  team_tweets_df.loc[team_tweets_df[\"Team\"] == \"Columbus Blue Jackets\"]\n",
    "# tokenized_tweets = sample_team_df[\"Tweet\"].tolist()\n",
    "# dictionary = corpora.Dictionary(tokenized_tweets)\n",
    "# doc_term_matrix = [dictionary.doc2bow(rev) for rev in tokenized_tweets]\n",
    "\n",
    "\n",
    "# LDA = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "# lda_model = LDA(corpus=doc_term_matrix, id2word=dictionary, num_topics=3, random_state=100, chunksize=100, passes=50,iterations=100) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd1d7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer='word',       \n",
    "                             min_df=10,                        # minimum reqd occurences of a word \n",
    "                             stop_words='english',             # remove stop words\n",
    "                             lowercase=True,                   # convert all words to lowercase\n",
    "                             #ngram_range=(1)\n",
    "                             #token_pattern='[a-zA-Z0-9]{3,}',  # num chars > 3\n",
    "                             # max_features=50000,             # max number of uniq words\n",
    "                            )\n",
    "\n",
    "data_vectorized = vectorizer.fit_transform(sample_team_df[\"Tweet\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ca1805",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the sparsity of the data\n",
    "# Materialize the sparse data\n",
    "data_dense = data_vectorized.todense()\n",
    "\n",
    "# Compute Sparsicity = Percentage of Non-Zero cells\n",
    "print(\"sparsity: \", ((data_dense > 0).sum()/data_dense.size)*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56af8c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Search Param\n",
    "search_params = {'n_components': [10, 15, 20, 25, 30], 'learning_decay': [.5, .7, .9]}\n",
    "\n",
    "# Init the Model\n",
    "lda = LatentDirichletAllocation()\n",
    "\n",
    "# Init Grid Search Class\n",
    "model = GridSearchCV(lda, param_grid=search_params)\n",
    "\n",
    "# Do the Grid Search\n",
    "model.fit(data_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328f5218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Model\n",
    "best_lda_model = model.best_estimator_\n",
    "\n",
    "# Model Parameters\n",
    "print(\"Best Model's Params: \", model.best_params_)\n",
    "\n",
    "# Log Likelihood Score\n",
    "print(\"Best Log Likelihood Score: \", model.best_score_)\n",
    "\n",
    "# Perplexity\n",
    "print(\"Model Perplexity: \", best_lda_model.perplexity(data_vectorized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88550708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Log Likelyhoods from Grid Search Output\n",
    "n_topics = [10, 15, 20, 25, 30]\n",
    "#print(model.cv_results_)\n",
    "scores_mean = model.cv_results_['mean_test_score'].tolist()\n",
    "print(scores_mean)\n",
    "log_likelyhoods_5 = scores_mean[0:5]\n",
    "log_likelyhoods_7 = scores_mean[5:10]\n",
    "log_likelyhoods_9 = scores_mean[10:15]\n",
    "\n",
    "# Show graph\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(n_topics, log_likelyhoods_5, label='0.5')\n",
    "plt.plot(n_topics, log_likelyhoods_7, label='0.7')\n",
    "plt.plot(n_topics, log_likelyhoods_9, label='0.9')\n",
    "plt.title(\"Choosing Optimal LDA Model\")\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Log Likelyhood Scores\")\n",
    "plt.legend(title='Learning decay', loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7637e841",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pyLDAvis.enable_notebook()\n",
    "# vis = gensimvis.prepare(best_lda_model, data_vectorized, vectorizer)\n",
    "# vis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b39aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.sklearn\n",
    "pyLDAvis.enable_notebook()\n",
    "panel = pyLDAvis.sklearn.prepare(best_lda_model, data_vectorized, vectorizer, mds='tsne')\n",
    "panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3790da28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_topics(vectorizer=vectorizer, lda_model=best_lda_model, n_words=20):\n",
    "    keywords = np.array(vectorizer.get_feature_names())\n",
    "    topic_keywords = []\n",
    "    for topic_weights in lda_model.components_:\n",
    "        top_keyword_locs = (-topic_weights).argsort()[:n_words]\n",
    "        topic_keywords.append(keywords.take(top_keyword_locs))\n",
    "    return topic_keywords\n",
    "\n",
    "topic_keywords = show_topics(vectorizer=vectorizer, lda_model=best_lda_model, n_words=15)        \n",
    "\n",
    "# Topic - Keywords Dataframe\n",
    "df_topic_keywords = pd.DataFrame(topic_keywords)\n",
    "df_topic_keywords.columns = ['Word '+str(i) for i in range(df_topic_keywords.shape[1])]\n",
    "df_topic_keywords.index = ['Topic '+str(i) for i in range(df_topic_keywords.shape[0])]\n",
    "df_topic_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0328bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "lda_output = best_lda_model.transform(data_vectorized)\n",
    "svd_model = TruncatedSVD(n_components=2)  # 2 components\n",
    "lda_output_svd = svd_model.fit_transform(lda_output)\n",
    "\n",
    "# X and Y axes of the plot using SVD decomposition\n",
    "x = lda_output_svd[:, 0]\n",
    "y = lda_output_svd[:, 1]\n",
    "clusters = KMeans(n_clusters=10, random_state=100).fit_predict(lda_output)\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.scatter(x, y, c=clusters)\n",
    "plt.xlabel('Component 2')\n",
    "plt.xlabel('Component 1')\n",
    "plt.title(\"Segregation of Topic Clusters\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f4e9fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
